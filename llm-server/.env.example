# LLM Provider Configuration
# Choose your LLM provider: openai, lmstudio, localai
LLM_PROVIDER=openai

# API Configuration
# For OpenAI: use 'https://api.openai.com/v1'
# For LM Studio: default is 'http://localhost:1234/v1'
# For LocalAI: configure based on your setup
LLM_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=your_api_key_here

# Model Configuration
# OpenAI models: gpt-4, gpt-3.5-turbo, etc.
# LM Studio/LocalAI: depends on your loaded model
MODEL_NAME=gpt-3.5-turbo

# Generation Parameters
MAX_TOKENS=1000
TEMPERATURE=0.7

# Server Configuration
INACTIVITY_TIMEOUT_MINUTE=3
# Conversation Settings
MAX_CONVERSATION_TURNS=20
CONVERSATION_BATCH_SIZE=5

# System Settings
DEBUG_MODE=false
LOG_LEVEL=info
