# GPT-Conversation-Generator

This repository is used for reference in thesis paper "Enhancing Language Model Efficiency through Sequence-Level Knowledge Distillation with Sparse Transformers" by Ronald John Atanoso and Dan Victor Lofranco from Data Collection Page

The purpose for this repository is to be used for peer review on our thesis or use this as a tool in collecting synthetic conversational data generated by these large language models.

---

## Table of Contents

- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Running the System](#running-the-system)
- [Data Collection Workflow](#data-collection-workflow)
- [Troubleshooting](#troubleshooting)
- [FAQ](#faq)
- [License](#license)

---

## Project Structure

```
GPT-Conversation-Generator/
├── backend/                # Flask API, DB models, and data collection logic
│   ├── app.py
│   ├── scriptcron.py
│   ├── requirements.txt
├── llm-server/             # Node.js LLM API wrapper (OpenAI/LM Studio)
│   ├── index.js
│   ├── package.json
│   ├── package-lock.json
├── lrunner.sh              # Main process runner script
├── .env.example            # Example environment configuration
├── README.md
```

---

## Requirements

- Python 3.10+
- Node.js 18+
- PostgreSQL (for conversation storage)
- [LM Studio](https://lmstudio.ai/) or OpenAI-compatible LLM API
- `pm2` (for process management, install via `npm i -g pm2`)
- Linux or WSL recommended

---

## Quick Start

1. **Clone the repository:**

   ```bash
   git clone https://github.com/yourusername/GPT-Conversation-Generator.git
   cd GPT-Conversation-Generator
   ```

2. **Set up the environment:**

   - Copy `.env.example` to `.env` and edit as needed:
     ```bash
     cp .env.example .env
     ```
   - Update `DATABASE_URL`, `LLM_BASE_URL`, `MODEL_NAME`, etc.

3. **Install Python dependencies:**

   ```bash
   cd backend
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   ```

4. **Install Node.js dependencies:**

   ```bash
   cd ../llm-server
   npm install
   ```

5. **Set up PostgreSQL database:**
   - Ensure PostgreSQL is running.
   - Create the database as specified in your `.env` (`conversation-generator` by default).
   - The tables will be auto-created on first run.

---

## Configuration

Edit the `.env` file in the project root. Key variables:

- `LLM_PROVIDER`: `lmstudio` or `openai`
- `LLM_BASE_URL`: URL of your LLM API (e.g., `http://localhost:1234/v1`)
- `LLM_API_KEY`: API key if required (LM Studio default: `sk-no-key-required`)
- `MODEL_NAME`: Model name loaded in LM Studio or OpenAI
- `DATABASE_URL`: PostgreSQL connection string
- `MAX_TOKENS`, `TEMPERATURE`, etc.: LLM generation parameters

---

## Running the System

**Start all services and data collection:**

```bash
./lrunner.sh
```

- This script:
  - Activates the Python virtual environment
  - Starts the Flask backend (port 8000)
  - Launches multiple LLM server instances (default: 12, ports 8080+)
  - Starts the data collection cron (`scriptcron.py`)
  - Handles process cleanup and restarts

**To stop:**  
Press `Ctrl+C` in the terminal running `lrunner.sh`. All processes will be stopped and cleaned up.

---

## Data Collection Workflow

1. **Flask backend** (`backend/app.py`):

   - Orchestrates conversation generation between two LLM servers.
   - Stores conversation pairs in the PostgreSQL database.

2. **LLM servers** (`llm-server/index.js`):

   - Wraps LM Studio/OpenAI API with a REST interface.
   - Handles chat sessions, prompt/response, and basic cleaning.

3. **Data collection script** (`backend/scriptcron.py`):

   - Periodically triggers conversation generation between LLM servers.
   - Supports multiple topics and randomization.

4. **Database**:
   - Stores each conversation pair (start/end) with timestamps.

---

## Troubleshooting

- **Flask or LLM server won't start:**

  - Check `.env` for correct ports and database URL.
  - Ensure LM Studio is running and API is enabled.
  - Check logs: `backend/app.log`, `pm2 logs`.

- **Database errors:**

  - Ensure PostgreSQL is running and accessible.
  - Check credentials and database name in `.env`.

- **LLM not responding:**

  - Confirm LM Studio/OpenAI endpoint is reachable.
  - Check `LLM_BASE_URL` and `MODEL_NAME`.

- **Port conflicts:**
  - Make sure ports 8000, 8080-8091 (default) are free.

---

## FAQ

**Q: Can I use OpenAI instead of LM Studio?**  
A: Yes, set `LLM_PROVIDER=openai` and provide your OpenAI API key and model.

**Q: Where is the data stored?**  
A: In the PostgreSQL database specified by `DATABASE_URL`.

**Q: How do I change the number of LLM servers?**  
A: Edit `num_server` in `lrunner.sh` and adjust accordingly.

**Q: How do I add new conversation topics?**  
A: Edit the `chatSettings` list in `backend/scriptcron.py`.
